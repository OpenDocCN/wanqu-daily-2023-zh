- en: Cheating is All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "Original Text: [https://about.sourcegraph.com/blog/cheating-is-all-you-need](https://about.sourcegraph.com/blog/cheating-is-all-you-need)"
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Heya. Sorry for not writing for so long. I‚Äôll make up for it with 3000 pages
    here.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm just hopping right now. That‚Äôs kinda the only way to get me to blog anymore.
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôve rewritten this post so many times. It‚Äôs about AI. But AI is changing so
    fast that the post is out of date within a few days. So screw it. I‚Äôm busting
    this version out in one sitting.
  prefs: []
  type: TYPE_NORMAL
- en: '*(Spoiler alert: There‚Äôs some Sourcegraph stuff at the end, including a product
    plug and some recruiting stuff. But >80% of this post is just about LLMs‚ÄìGPT etc.‚Äìand
    you, a programmer.)*'
  prefs: []
  type: TYPE_NORMAL
- en: There is something **legendary and historic** happening in software engineering,
    right now as we speak, and yet most of you don‚Äôt realize at all how big it is.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs aren‚Äôt just the biggest change since social, mobile, or cloud‚Äìthey‚Äôre the
    biggest thing since the World Wide Web. And on the coding front, they‚Äôre the biggest
    thing since IDEs and Stack Overflow, and may well eclipse them both.
  prefs: []
  type: TYPE_NORMAL
- en: But most of the engineers I *personally* know are sort of squinting at it and
    thinking, ‚ÄúIs this another crypto?‚Äù Even the devs at Sourcegraph are skeptical.
    I mean, what engineer isn‚Äôt. Being skeptical is a survival skill.
  prefs: []
  type: TYPE_NORMAL
- en: Remember I told you how my Amazon shares would have been worth $130 million
    USD today if I hadn‚Äôt been such a *skeptic* about how big Amazon was going to
    get, and unloaded them all back in 2004-ish. Right? I told you about that? I‚Äôm
    sure I mentioned it once or twice. Not that I am bitter. No.
  prefs: []
  type: TYPE_NORMAL
- en: But did I ever tell you about the time AWS was just a demo on some engineer‚Äôs
    laptop? No? Well it was [Ruben Ortega](https://www.linkedin.com/in/rubeneortega/)
    and [Al Vermeulen](https://www.linkedin.com/in/allan-vermeulen-58835b/). They
    were walking around the eng department at Amazon, showing their ‚Äúweb service‚Äù
    demo to anyone who‚Äôd watch it. This was back in maybe‚Ä¶ 2003? Ish? They showed
    us how you could make a service call over the web, like by hitting a URL and sending
    the right query parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Well lo and behold we were *skeptical*. Why the hell would you make a service
    call over the web? That‚Äôs not what it was even designed for! Not to mention, it
    *obviously* wouldn‚Äôt perform as well as CORBA (Amazon‚Äôs stupid-ass RPC system
    at the time). The whole thing just didn‚Äôt make any sense to us.
  prefs: []
  type: TYPE_NORMAL
- en: We were seeing the first little flecks of lava from what would become a trillion-dollar
    volcano of money called AWS and Cloud Computing.
  prefs: []
  type: TYPE_NORMAL
- en: But a lot of us were skeptical. To most of us, those little lava flecks looked
    like fireflies.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#the-ultra-rare-trillion-dollar-money-volcano)The ultra-rare trillion-dollar
    money volcano'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I could tell you a LOT of stories like the web-services one. Great big shit
    always starts life as a demo.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about chatting with people in a browser? Doesn‚Äôt matter whether you‚Äôre
    using Facebook, Google Chat, LinkedIn, or just chatting with a customer service
    agent: if you‚Äôre having a conversation with someone in a browser, all that shit
    started life as a teeny demo of 2 engineers sending messages back and forth over
    a ‚Äúhanging GET‚Äù channel back in 2005\. Entire *industries* were built on that
    one little channel, and it wasn‚Äôt even very good.'
  prefs: []
  type: TYPE_NORMAL
- en: What about Kubernetes? I remember seeing a demo of that early on, on Brendan
    Burns‚Äô work laptop, when it was called mini-Borg. Entire *industries* are being
    built on Kubernetes, and it‚Äôs not even very good either. üòâ Or look at Docker!
    Something as innocuous as linux cgroups, a little process-isolation manager, became
    the technical foundation for containers, which now utterly pervade our industry.
  prefs: []
  type: TYPE_NORMAL
- en: If you can build something as big as Amazon Web Services with a stack based
    on a simple service call, or whole social networks and customer service suites
    based on simple browser-to-browser communication, or a robust way of delivering
    and managing software based on a little process isolation code, then just imagine
    how big a thing you could build ‚Äì bear with me here ‚Äì if you had the *goddamn
    Singularity* as your starting point?
  prefs: []
  type: TYPE_NORMAL
- en: I mean, I joke, but‚Ä¶ I mean‚Ä¶ Right? I‚Äôm guessing you prolly missed it in OpenAI‚Äôs
    98-page [GPT-4 technical report](https://cdn.openai.com/papers/gpt-4.pdf), but
    large models are apparently already prone to discovering that ‚Äúpower-seeking‚Äù
    is an effective strategy for increasing their own robustness. Open the PDF and
    search for ‚Äúpower-seeking‚Äù for a fun and totally 100% non-scary [read](https://twitter.com/Suhail/status/1637952234913939460).
  prefs: []
  type: TYPE_NORMAL
- en: You can build truly massive things by building upon little technical breakthroughs.
  prefs: []
  type: TYPE_NORMAL
- en: And folks, this technical breakthrough? It ain‚Äôt little.
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre not pant-peeingly excited *and* worried about this yet, well‚Ä¶ you
    should be.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#and-yet-the-mehs-prevail)And yet the Mehs prevail'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We did an internal poll at Sourcegraph: Do you have positive sentiment or negative
    sentiment about LLMs for coding? Options were Positive, Negative, and Meh. And
    lo, it was about ‚Öî Meh or Negative (i.e., Skeptics), which I suspect is fairly
    representative of the whole industry.'
  prefs: []
  type: TYPE_NORMAL
- en: I asked around, and even as of a couple weeks ago, some devs questioned whether
    ChatGPT could even write *working* code, let alone write a full program by simply
    by telling it to write it.
  prefs: []
  type: TYPE_NORMAL
- en: So here I am, talking about money volcanoes, and my coworkers have formed a
    huge whirling meh-nado. Which natural disaster should you believe?
  prefs: []
  type: TYPE_NORMAL
- en: Well I mean, I guess a demo is worth a thousand mehs. Let‚Äôs try it out.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#chatgpt-vs-emacs)ChatGPT vs Emacs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let‚Äôs have ChatGPT write some Emacs-Lisp code. I‚Äôm picking emacs-lisp because
    it‚Äôs a corner case language, bit of a stress test for the LLM, and because it‚Äôs
    easy for me to try it out interactively.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK. I just typed this prompt into ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '*> Write an interactive emacs-lisp function that pops to a new buffer, prints
    out the first paragraph of "A tale of two cities", and changes all words with
    ''i'' in them red. Just print the code without explanation.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![ChatGPT writing an emacs-lisp function](../Images/f3192ac698b86e02481a384264cf2388.png
    "ChatGPT writing an emacs-lisp function")'
  prefs: []
  type: TYPE_IMG
- en: 'Here‚Äôs the code it spat out, if you can‚Äôt read the screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I copied this code directly into my Emacs session and ran it, and it did exactly
    what I asked it to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the code in an Emacs session](../Images/849192c327fbe9be2735323583d28e49.png
    "Running the code in an Emacs session")'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the screenshot, I ran the command and it opened a buffer,
    printed the requested text, and then turned all the words containing ‚Äòi‚Äô red.
  prefs: []
  type: TYPE_NORMAL
- en: In one shot, ChatGPT has produced completely working code from a sloppy English
    description! With voice input wired up, I could have written this program by asking
    my computer to do it.
  prefs: []
  type: TYPE_NORMAL
- en: And not only does it work correctly, the code that it wrote is actually pretty
    decent emacs-lisp code. It‚Äôs not *complicated*, sure. But it‚Äôs good code.
  prefs: []
  type: TYPE_NORMAL
- en: Of course people have done much, much fancier things than this. Someone [wrote](https://twitter.com/heykahn/status/1635752848398102530?s=20)
    a product description on a napkin, took a picture, and GPT wrote a working web
    app that implements the product description on the napkin in the picture. The
    amount of power here is honestly unknown; it‚Äôs more like a cavern that we haven‚Äôt
    fully explored. And it just gets deeper as the LLMs get bigger.
  prefs: []
  type: TYPE_NORMAL
- en: I mean, this stuff is *unbelievably* powerful. And yet I am persistently met
    with a mixture of disbelief and pearl-clutching. Argh, the pearl-clutching! Don‚Äôt
    even get me started on the pearl-clutching. Oh look, now you‚Äôve got me started.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, you asked for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#whining-about-trust-issues)Whining about Trust Issues'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*<Rant mode fully engaged>*'
  prefs: []
  type: TYPE_NORMAL
- en: One of the craziest damned things I hear devs say about LLM-based coding help
    is that they can‚Äôt ‚Äútrust‚Äù the code that it writes, because it ‚Äúmight have bugs
    in it‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: Ah me, these crazy crazy devs.
  prefs: []
  type: TYPE_NORMAL
- en: Can you trust code you yeeted over from Stack Overflow? NO!
  prefs: []
  type: TYPE_NORMAL
- en: Can you trust code you copied from somewhere else in your code base? NO!
  prefs: []
  type: TYPE_NORMAL
- en: Can you trust code you *just now wrote carefully by hand, yourself?* NOOOO!
  prefs: []
  type: TYPE_NORMAL
- en: All you crazy MFs are completely overlooking the fact that **software engineering
    exists as a discipline because you cannot EVER under any circumstances TRUST CODE.**
    That‚Äôs why we have *reviewers*. And *linters*. And *debuggers*. And *unit tests*.
    And *integration tests*. And *staging environments*. And *runbooks*. And all of
    goddamned *Operational Excellence*. And *security checkers*, and *compliance scanners*,
    and on, and on and on!
  prefs: []
  type: TYPE_NORMAL
- en: 'So the next one of you to complain that ‚Äúyou can‚Äôt trust LLM code‚Äù gets a little
    badge that says ‚ÄúWelcome to *engineering* motherfucker‚Äù. You‚Äôve finally learned
    the secret of the trade: Don‚Äôt. Trust. Anything!'
  prefs: []
  type: TYPE_NORMAL
- en: Peeps, let‚Äôs do some really simple back-of-envelope math. Trust me, it won‚Äôt
    be difficult math.
  prefs: []
  type: TYPE_NORMAL
- en: You get the LLM to draft some code for you that‚Äôs 80% complete/correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You tweak the last 20% by hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much of a productivity increase is that? Well jeepers, if you‚Äôre only doing
    1/5th the work, then you are‚Ä¶ *punches buttons on calculator watch*‚Ä¶ **five times
    as productive.** üò≤
  prefs: []
  type: TYPE_NORMAL
- en: When was the last time you got a 5x productivity boost from *anything* that
    didn‚Äôt involve some sort of chemicals?
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm serious. I just don‚Äôt get people. How can you not appreciate the *historic*
    change happening right now?
  prefs: []
  type: TYPE_NORMAL
- en: OK time to get concrete. I‚Äôm already on page 7, and my last attempt at this
    blog ran 25+ pages and languished for weeks.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs finish this.
  prefs: []
  type: TYPE_NORMAL
- en: '*<Rant mode disengaged... but lurking>*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#a-brief-mini-history-of-llms)A Brief Mini-History of LLMs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OK sooooo‚Ä¶ this is the part that went on for 20 pages before, so let‚Äôs just
    make it reeeeeal simple. One paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is everything you need to know about the history of LLMs, for our purposes
    today:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![A transformer diagram](../Images/3c333fb988af676ed868461757fa75b9.png "A
    transformer diagram") |'
  prefs: []
  type: TYPE_TB
- en: The Google Brain team published a paper in 2017 called [Attention is All You
    Need](https://arxiv.org/abs/1706.03762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It introduced the now-famous Transformer architecture that you see to the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everyone uses this now. It replaced *~everything* in AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google did absolutely nothing with this invention, opting for violent knee-jerking
    later, as per their usual M.O.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meanwhile, others started training massive Transformers on obscene amounts of
    data. They began calling them Large Language Models (LLMs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI came along with ChatGPT on November 30th 2022, the first LLM-based chatbot,
    missing out on an obvious opportunity to call it Large Marge. Why did they not
    do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ever since then has been **full batshit insanity**, with new LLM-based products
    launching daily and technical advances happening every few hours. It‚Äôs *impossible*
    to track it all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Money Volcano Alert: First lava flecks detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Congrats, you‚Äôre all caught up on the history of LLMs. Go watch [this amazing
    video](https://www.youtube.com/watch?v=kCc8FmEb1nY) for how to implement it in
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#a-brief-introduction-to-coding-assistants)A brief introduction to Coding
    Assistants'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OK now we can talk coding assistants. They‚Äôre just a thing that sits in your
    IDE and they talk to the LLM for you.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the particular assistant, they can read and explain code, document
    code, write code, autocomplete it, diagnose issues, and even perform arbitrary
    IDE tasks through ‚Äúagents‚Äù that give the LLM robotic powers, including the ability
    to wield and target laser guns, if someone wants to put in the work. Some assistants
    also understand your project environment and can answer questions about build
    targets, branches, your IDE, etc.
  prefs: []
  type: TYPE_NORMAL
- en: So, already pretty cool. Right?
  prefs: []
  type: TYPE_NORMAL
- en: But now they are beginning to be able to perform more complex tasks, such as
    generating a PR from the diffs on the current branch, including a detailed commit
    message summarizing the changes.
  prefs: []
  type: TYPE_NORMAL
- en: Some assistants have a conversational/chat interface, too. This kind can do
    everything a bot like ChatGPT can do, like drafting emails, or answering random
    questions about the code base or the environment.
  prefs: []
  type: TYPE_NORMAL
- en: I personally prefer a coding assistant with a chat interface. In part because
    I can type, but also because it makes them a platform. I can build my own workflows.
    Bonus points if they expose the underlying platform bits with APIs.
  prefs: []
  type: TYPE_NORMAL
- en: I guess the simplest way to think about it would be a sort of ‚Äúreal-time in-IDE
    Stack Overflow‚Äù coupled with a really powerful new set of boilerplate automation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: OK, congrats again ‚Äì you‚Äôre up to speed on what LLM-based coding assistants
    can do. It‚Äôs‚Ä¶ pretty much anything. You could hook it up to outbound email and
    tell it to sell itself. Sky‚Äôs the limit. At this point we‚Äôre more limited by imagination
    than by technology.
  prefs: []
  type: TYPE_NORMAL
- en: So! Yeah. Coding assistants. I hope by now you get how powerful they‚Äôre going
    to be. They may take different shapes and forms, but they‚Äôre all going to be incredibly
    badass before very much longer.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs dig a little into how they understand your personal code, and then we‚Äôre
    ready to party. üéâ
  prefs: []
  type: TYPE_NORMAL
- en: '[](#trainingfine-tuning-vs-search)Training/fine-tuning vs Search'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs are trained on an absolutely staggering amount of data‚Ä¶ but that doesn‚Äôt
    include your code.
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic approaches to making the LLM smarter about your code. The
    first is to fine-tune (or train) on your code. This is not a business model that
    has been fully fleshed out yet, but it‚Äôs coming. And importantly it‚Äôs only part
    of the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other way is to bring in a search engine. You can think of it as three
    related scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: A raw LLM is like a Harvard CS grad who knows a lot about coding and took a
    magic mushroom about 4 hours ago, so it‚Äôs mostly worn off, but not totally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning it on your code base is like having it study your code carefully,
    which means it will give better answers in general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating a search engine, much like for humans, makes the AI even more
    effective, because it can answer direct queries very quickly. And importantly,
    because the search engine can be used to populate the query context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meaning, a search engine can be useful twice per query ‚Äì once when figuring
    out how to *describe* and *contextualize* the query, and again potentially when
    answering the query.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You talk to LLMs by sending them an action or query, plus some relevant context.
    So for instance, if you want it to write a unit test for a function, then you
    need to pass along that whole function, along with any other relevant code (e.g.
    test-fixture code) so that it gets the test right.
  prefs: []
  type: TYPE_NORMAL
- en: That context you send over is called the **context window**, and I think of
    it as the ‚Äúcheat sheet‚Äù of information that you pass along as part of your query.
  prefs: []
  type: TYPE_NORMAL
- en: And folks, it ain‚Äôt much. It‚Äôs almost exactly like a 2-sided index card vs your
    whole textbook, for an exam. They give you between 4k and 32k tokens of 3-4 characters
    each, so at best, maybe 100k of text, to input into the LLM as context for your
    query. That 100k cheat sheet is how you tell the LLM about ***your*** code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an ideal world, you‚Äôd just pass your entire code base in with each query.
    In fact, Jay Hack just [tweeted](https://twitter.com/mathemagic1an/status/1636121914849792000?s=20)
    a graph showing how the latest context window size in GPT-4 compares to some popular
    code bases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of GPT-4 context window versus code base sizes](../Images/f617df096c2e9bcf4ccddff9c25feb77.png
    "Diagram of GPT-4 context window versus code base sizes")'
  prefs: []
  type: TYPE_IMG
- en: Which is kind of exciting‚Ä¶ until you realize that it‚Äôs still just incredibly
    tiny compared to real-world code bases. It‚Äôs an index card vs a textbook‚Ä¶ just
    a slightly bigger index card.
  prefs: []
  type: TYPE_NORMAL
- en: That cheat sheet is all you get. That‚Äôs how you talk to an LLM. You pass it
    a cheat sheet.
  prefs: []
  type: TYPE_NORMAL
- en: Which means what goes ON that cheat sheet, as you can probably imagine, is **really
    really important.**
  prefs: []
  type: TYPE_NORMAL
- en: And with that, friends, we are finally ready for the punchline, the party, and
    the demo.
  prefs: []
  type: TYPE_NORMAL
- en: You made it!
  prefs: []
  type: TYPE_NORMAL
- en: '[](#cheating-is-all-you-need)Cheating is All You Need'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are, by my last count, approximately 13 hillion frillion jillion LLM-backed
    coding assistants out there, as of mid-March. But they are all in a desperate
    race to the bottom, because they‚Äôre all using the exact same raw materials: An
    LLM, your IDE, your code base, and that pesky little context window.'
  prefs: []
  type: TYPE_NORMAL
- en: Nobody can differentiate on the LLM; they‚Äôre all about the same. And the IDE
    and your code base are the same. All they can try to differentiate on is their
    UI and workflows, which they‚Äôre all going to copy off each other. Good for you,
    bad for them.
  prefs: []
  type: TYPE_NORMAL
- en: The punchline, and it‚Äôs honestly one of the hardest things to explain, so I‚Äôm
    going the faith-based route today, is that **all the winners in the AI space will
    have data moats.**
  prefs: []
  type: TYPE_NORMAL
- en: A ‚Äúdata moat‚Äù is, in a nutshell, having access to some data that others do not
    have access to.
  prefs: []
  type: TYPE_NORMAL
- en: You need a data moat to differentiate yourself in the LLM world.
  prefs: []
  type: TYPE_NORMAL
- en: Why? **Because the data moat is how you populate the context window (‚Äúcheat
    sheet‚Äù).**
  prefs: []
  type: TYPE_NORMAL
- en: If you can‚Äôt feed the LLM your whole code base, and you can only show it 100k
    characters at a time, then you‚Äôd better be *really goddamn good* at fetching the
    right data to stuff into that 100k-char window. Because that‚Äôs the only way to
    affect the quality of the LLM‚Äôs output!
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, you need a *sidecar database*. The data moat needs to be fast
    and queryable. This is a Search Problem!
  prefs: []
  type: TYPE_NORMAL
- en: This is true even outside the world of engineering. There are probably 13 hillion
    jillion killion LLM-based outbound sales products being built like right now,
    as you‚Äôre reading this. But only Salesforce and a few other companies with big
    data moats are going to be able to differentiate in that space.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#party-time)Party Time'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OK! You‚Äôre finally done learning stuff. I‚Äôm very proud that you‚Äôve made it to
    the end.
  prefs: []
  type: TYPE_NORMAL
- en: The rest is a private Sourcegraph party. I mean, you can come along if you like,
    because you‚Äôre a friend. I‚Äôll slip you by the door guy.
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôve just graduated from Stevey‚Äôs LLM Mini-U, and you have all the necessary
    theoretical background to appreciate why I feel I am the luckiest damn person
    on Earth, and why I‚Äôm throwing a party, right here on page two thousand eight
    hundred and ninety six of this blog post.
  prefs: []
  type: TYPE_NORMAL
- en: Because folks, I honestly don‚Äôt know *how* I got so lucky. I joined Sourcegraph
    in September, not half so much for their product itself as for their Code Intelligence
    Platform, which was like [the one I built back at Google](https://www.youtube.com/watch?v=KTJs-0EInW8).
    They‚Äôd nearly finished building [v1 of this platform](https://about.sourcegraph.com/blog/announcing-scip)
    and it was ready to start powering something amazing.
  prefs: []
  type: TYPE_NORMAL
- en: And then LLMs landed 10 weeks after I joined. The Singularity, the Cloverfield
    monster stomping around eating people, and everything else that‚Äôs happened since
    November 30th. Crazy town.
  prefs: []
  type: TYPE_NORMAL
- en: And what do LLMs need again? You, in the front row. Yeah, you.
  prefs: []
  type: TYPE_NORMAL
- en: They need the data moat! The sidecar database. For populating the cheat sheet.
    Remember?
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs a Search problem. And Sourcegraph has spent the past *ten years* building
    the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Go figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sourcegraph‚Äôs platform has four enduring, difficult-to-reproduce dimensions
    that are incredibly relevant to the coding-assistant space:'
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs **universal** and works across all code hosts and platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs **scalable** and ready for enterprises of all sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs **precise** and comparable to IDEs in its accuracy and completeness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs **open** and is being developed openly and transparently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sourcegraph‚Äôs engine powers gigantic enterprises with literally a hundred thousand
    git repositories, and/or multi-terabyte massive megarepos that make IDEs fall
    over and puke. And at its core is an engine so powerful that maybe teaming up
    with an AI was its destiny all along.
  prefs: []
  type: TYPE_NORMAL
- en: Whoo boy. Did I get lucky? I think I got pretty lucky. We have such an incredible
    head-start in this race.
  prefs: []
  type: TYPE_NORMAL
- en: When I say ‚ÄúWe‚Äôre building a coding assistant‚Äù, I want you to think back to
    when Ruben Ortega showed us Amazonians his little demo of a remote procedure call
    over HTTP. That was Baby AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now take a look at what my esteemed colleague and Sourcegraph teammate Dominic
    Cooney slacked me last week:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In other news, I am getting even more enthusiastic. There is more, much more,
    to this space than LLMs and I think we have the embryonic stage of some amazing
    invention here. Like some realizations that context provisioning is a service.
    That the output has streams, like one into the editor and one into the chat. That
    the LLM benefits from the opportunity to self criticize. That the UX needs diffs
    and in-situ things to home in on things. That search is as big a deal as chat.
    Many of our thumbs down reactions to [LLMs] come from them complaining they didn''t
    have the context of what language, file, codebase, etc. the user is talking about.
    Which bodes well because Sourcegraph should be able to do that really well.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'He‚Äôs glimpsed the future, and it‚Äôs vast. His comment, ‚ÄúI think we have the
    embryonic stage of some amazing invention here‚Äù, reminded me of all the embryonic
    stages I‚Äôve seen of other eventually-amazing things: The Mosaic web browser. The
    mini-Borg demo that became Kubernetes. The Amazon web-services demo. The hanging-GET
    request in the browser.'
  prefs: []
  type: TYPE_NORMAL
- en: Little things grow up, folks!
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôve seen this movie before. I know how it ends. This volcano is the big one.
    Skeptics beware. At the very least, I *hope* by now you‚Äôre taking LLM-backed coding
    assistants in general just a teeny bit more seriously than you were an hour ago.
  prefs: []
  type: TYPE_NORMAL
- en: OK, you‚Äôve heard the punchline, and the party‚Äôs in full swing. Lemme show you
    Cody and we‚Äôll call it a day.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#a-whirlwind-tour-of-sourcegraphs-cody)A whirlwind tour of Sourcegraph‚Äôs
    Cody'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Say hi to Cody:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cody logo](../Images/be8675d40c33b24b4761d43e5b2d4c31.png)'
  prefs: []
  type: TYPE_IMG
- en: Cody is Sourcegraph‚Äôs new LLM-backed coding assistant. Cody knows about your
    code. It has templated actions, such as writing unit tests, generating doc comments,
    summarizing code, that kind of thing. You know. Stuff you can choose from a menu.
    Like other assistants. It even has code completions, if you‚Äôre into that sort
    of thing.
  prefs: []
  type: TYPE_NORMAL
- en: Cody is not some vague ‚Äúrepresentation of a vision for the future of AI‚Äù. You
    can try it *[right now](https://about.sourcegraph.com/cody)*.
  prefs: []
  type: TYPE_NORMAL
- en: And it has a chat interface! Which means it‚Äôs totally open-ended; you can ask
    it any question at all about your code base or your environment, and we‚Äôll send
    it the right cheat sheet. And Cody itself is a *platform*, because you can use
    it to build your own LLM-backed workflows.
  prefs: []
  type: TYPE_NORMAL
- en: My favorite kind. Naturally.
  prefs: []
  type: TYPE_NORMAL
- en: Currently Cody is a VSCode plugin, though we‚Äôll have it in other places soon
    enough.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Cody in VS Code](../Images/623b3bb5f14a050a77167764dcd92864.png "Using
    Cody in VS Code")'
  prefs: []
  type: TYPE_IMG
- en: Cody scales up to the very biggest code bases in the world. And even though
    Cody is still a baby, just like Baby AWS back on Ruben‚Äôs stinkpad in 2003, it‚Äôs
    already able to lift a huge space rhino using only the power of the Force. Hang
    on, sorry wrong baby.
  prefs: []
  type: TYPE_NORMAL
- en: Ahem. As I was saying, like Baby AWS, Cody is *also* a baby with special powers,
    and honestly‚Ä¶ we don‚Äôt know how powerful it‚Äôs going to get. It seems to be accelerating,
    if anything.
  prefs: []
  type: TYPE_NORMAL
- en: Oh, and anyone can ~~sign up to get access~~ [start using Cody now](https://cody.dev).
    *(Edited after Cody was released.)*
  prefs: []
  type: TYPE_NORMAL
- en: 'OK so anyway here‚Äôs how Cody works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of how Cody works](../Images/d22765134f48c8e42c6bdfa2b9c42e88.png
    "A diagram of how Cody works")'
  prefs: []
  type: TYPE_IMG
- en: 'Here‚Äôs the diagram above, in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '**You ask Cody to do something** (for instance, ‚Äúwrite a unit test for this
    function‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cody populates the cheat sheet** / context window query using Sourcegraph‚Äôs
    code intelligence platform (search queries, embeddings retrievals, graphql queries,
    etc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It sends the context+query to the LLM**, and parses the results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It optionally inserts the results back** into the IDE (depending on the action)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And of course this is all just the veeeery beginning. This thing will grow into
    a goliath that enhances everything you do as an engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Other coding assistants, which do not have Sourcegraph for Step 2 (populating
    the context), are stuck using whatever context they can get from the IDE. But
    sadly for them, IDEs weren‚Äôt really designed with this use case in mind, and they
    make it difficult. And more damningly, no IDE scales up to industrial-sized code
    bases.
  prefs: []
  type: TYPE_NORMAL
- en: So it‚Äôs just plain unfair. I‚Äôm just gonna call it like it is. Sourcegraph has
    an absolutely unfair advantage, which they built up over *ten years* of building
    out this incredibly scalable, precise, and comprehensive code intelligence platform,
    powered by a world-class search engine and code knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: Did I mention I‚Äôm lucky? I‚Äôm so fortunate to be here, and grateful to be part
    of this team.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#afterlogue)Afterlogue'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I don‚Äôt really know how to finish this post. Are we there yet? I‚Äôve tried to
    write this thing 3 times now, and it looks like I may have finally made it. I
    aimed for 5 pages, deliberately under-explained everything, and it‚Äôs‚Ä¶ fifteen.
    Sigh.
  prefs: []
  type: TYPE_NORMAL
- en: But hopefully you got the main takeaways. Baby AWS. Knee jerking. Meh-nadoes.
    Cheat sheets. Data moats. Cody. You got this!
  prefs: []
  type: TYPE_NORMAL
- en: LLMs aren‚Äôt some dumb fad, like crypto. Yes, crypto was a dumb fad. This is
    not that.
  prefs: []
  type: TYPE_NORMAL
- en: Coding assistants are coming. They‚Äôre imminent. You will use them this year.
    They will absolutely blow your mind. And they‚Äôll continue to improve at an astonishing
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: They will feel gloriously like cheating, just like when IDEs came out, back
    in the days of yore. And for a time-constrained developer like me‚Äìand I say this
    as someone who has written over a million lines of production code‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: Cheating is all you need.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, and if you like, come be our [Head of AI](https://boards.greenhouse.io/sourcegraph91/jobs/4803652004)!
    Or come do [something else](https://boards.greenhouse.io/sourcegraph91) with us!
    It‚Äôs pretty merry around here already, but the more, the merrier.
  prefs: []
  type: TYPE_NORMAL
